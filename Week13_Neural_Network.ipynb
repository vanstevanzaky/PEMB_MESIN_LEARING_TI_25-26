{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vanstevanzaky/PEMB_MESIN_LEARING_TI_25-26/blob/main/Week13_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JS13 - ARTIFICIAL NEURAL NETWORK (ANN) DAN EVALUASI CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praktikum 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praktikum ini bertujuan untuk membuat JST sederhana (2 layer) dengan forward pass dan backpropagation manual. Backpropagation adalah algoritma untuk melatih JST dengan mengoreksi kesalahan melalui perhitungan selisih antara keluaran jaringan dan target, lalu memperbarui bobot dan bias dari keluaran ke masukan untuk meminimalkan kesalahan. Langkah-langkahnya meliputi:\n",
    "1. Pembuatan dataset XOR\n",
    "2. Inisialisasi bobot dan bias\n",
    "3. Implementasi forward pass\n",
    "4. Perhitungan error dan backpropagation\n",
    "5. Pembaruan bobot menggunakan gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Buat dataset sederhana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tahap ini, kita akan membuat dataset XOR sederhana yang terdiri dari 4 input kombinasi (00, 01, 10, 11) dengan output yang sesuai dengan operasi XOR. Dataset ini digunakan sebagai data training untuk neural network sederhana yang akan kita buat. Selain itu, kita juga akan mendefinisikan parameter jaringan seperti jumlah neuron input, hidden layer, output, dan learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Dataset XOR\n",
    "X = np.array(([0,0],[0,1],[1,0],[1,1]))\n",
    "y = np.array(([0],[1],[1],[0]))\n",
    "#Parameters\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan:**\n",
    "- `X`: Input data dengan 4 kombinasi XOR (00, 01, 10, 11)\n",
    "- `y`: Target output XOR (0, 1, 1, 0)\n",
    "- `input_size`: 2 neuron input\n",
    "- `hidden_size`: 2 neuron pada hidden layer\n",
    "- `output_size`: 1 neuron output\n",
    "- `lr`: Learning rate 0.1 untuk mengontrol seberapa besar pembaruan bobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inisialisasi bobot dan bias\n",
    "\n",
    "Pada tahap ini, kita akan menginisialisasi bobot (weights) dan bias untuk kedua layer jaringan. Bobot diinisialisasi dengan nilai random menggunakan distribusi normal, sedangkan bias diinisialisasi dengan nilai nol. Selain itu, kita juga mendefinisikan fungsi aktivasi sigmoid dan turunannya yang akan digunakan dalam proses forward propagation dan backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan:**\n",
    "- `W1` dan `W2`: Bobot untuk layer 1 dan 2, diinisialisasi secara random\n",
    "- `b1` dan `b2`: Bias untuk layer 1 dan 2, diinisialisasi dengan nol\n",
    "- `sigmoid()`: Fungsi aktivasi untuk menghasilkan output antara 0 dan 1\n",
    "- `sigmoid_derivative()`: Turunan sigmoid untuk backpropagation\n",
    "\n",
    "**Hasil:** Bobot dan bias telah diinisialisasi. Tidak ada output yang ditampilkan, namun variabel W1, W2, b1, b2 telah tersimpan dalam memori dengan nilai random (untuk W) dan nol (untuk b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementasi forward pass\n",
    "### 4. Perhitungan error dan backpropagation\n",
    "### 5. Pembaruan bobot menggunakan gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tahap ini, kita akan melakukan training neural network dengan proses iteratif selama 10,000 epoch. Setiap epoch terdiri dari tiga tahap utama:\n",
    "1. **Forward pass**: menghitung output prediksi dari input\n",
    "2. **Backpropagation**: menghitung gradien error untuk setiap layer\n",
    "3. **Update weights**: memperbarui bobot dan bias menggunakan gradient descent\n",
    "\n",
    "Loss (Mean Squared Error) akan dicetak setiap 1000 epoch untuk memantau proses pembelajaran.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2970234658185617\n",
      "Epoch 1000, Loss: 0.23730865007887975\n",
      "Epoch 2000, Loss: 0.1623841071127087\n",
      "Epoch 3000, Loss: 0.047386345996047755\n",
      "Epoch 4000, Loss: 0.016020130427879302\n",
      "Epoch 5000, Loss: 0.008620018643256601\n",
      "Epoch 6000, Loss: 0.0057008908729233795\n",
      "Epoch 7000, Loss: 0.0041967231337173136\n",
      "Epoch 8000, Loss: 0.003294819533012895\n",
      "Epoch 9000, Loss: 0.0026993148636384762\n",
      "Prediksi:\n",
      "[[0.05115541]\n",
      " [0.95441808]\n",
      " [0.95432251]\n",
      " [0.0483417 ]]\n",
      "Epoch 5000, Loss: 0.008620018643256601\n",
      "Epoch 6000, Loss: 0.0057008908729233795\n",
      "Epoch 7000, Loss: 0.0041967231337173136\n",
      "Epoch 8000, Loss: 0.003294819533012895\n",
      "Epoch 9000, Loss: 0.0026993148636384762\n",
      "Prediksi:\n",
      "[[0.05115541]\n",
      " [0.95441808]\n",
      " [0.95432251]\n",
      " [0.0483417 ]]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan:**\n",
    "\n",
    "**Forward Pass:**\n",
    "- `z1 = X·W1 + b1`: Perhitungan linear layer 1\n",
    "- `a1 = sigmoid(z1)`: Aktivasi layer 1\n",
    "- `z2 = a1·W2 + b2`: Perhitungan linear layer 2\n",
    "- `a2 = sigmoid(z2)`: Output prediksi\n",
    "\n",
    "**Backpropagation:**\n",
    "- Hitung error dari selisih target (y) dan prediksi (a2)\n",
    "- `d_a2`: Gradien output layer\n",
    "- `d_W2, d_b2`: Gradien bobot dan bias layer 2\n",
    "- `d_a1`: Gradien hidden layer\n",
    "- `d_W1, d_b1`: Gradien bobot dan bias layer 1\n",
    "\n",
    "**Update Weights:**\n",
    "- Bobot dan bias diperbarui menggunakan gradient descent dengan learning rate 0.1\n",
    "**Analisis Output:**\n",
    "\n",
    "Setelah training 10,000 epoch, neural network menunjukkan:\n",
    "- **Loss berkurang secara bertahap**: Dari nilai tinggi di epoch awal menuju mendekati 0\n",
    "- **Prediksi final (a2)** akan mendekati target XOR:\n",
    "  - Input [0,0] → prediksi ≈ 0\n",
    "  - Input [0,1] → prediksi ≈ 1\n",
    "  - Input [1,0] → prediksi ≈ 1\n",
    "  - Input [1,1] → prediksi ≈ 0\n",
    "\n",
    "Network berhasil mempelajari pola XOR melalui proses backpropagation, membuktikan bahwa masalah non-linear XOR dapat diselesaikan dengan minimal 1 hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas 1: Eksperimen Hidden Layer dan Fungsi Aktivasi\n",
    "\n",
    "Pada tugas ini, kita akan melakukan eksperimen dengan:\n",
    "1. Mengubah jumlah neuron hidden layer dari 2 menjadi 3\n",
    "2. Membandingkan loss antara konfigurasi awal (2 neuron) dengan konfigurasi baru (3 neuron)\n",
    "3. Menambahkan fungsi aktivasi ReLU pada hidden layer dan membandingkan hasilnya dengan Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksperimen 1: Hidden Layer 3 Neuron (Sigmoid)\n",
    "\n",
    "Eksperimen pertama mengubah arsitektur jaringan dari 2 neuron menjadi 3 neuron pada hidden layer, dengan tetap menggunakan fungsi aktivasi Sigmoid. Tujuannya adalah membandingkan performa jaringan dengan kapasitas yang lebih besar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.390006153959982\n",
      "Epoch 1000, Loss: 0.17096910330188672\n",
      "Epoch 2000, Loss: 0.037171778080727164\n",
      "Epoch 3000, Loss: 0.01316416752896597\n",
      "Epoch 4000, Loss: 0.007367235687730666\n",
      "Epoch 1000, Loss: 0.17096910330188672\n",
      "Epoch 2000, Loss: 0.037171778080727164\n",
      "Epoch 3000, Loss: 0.01316416752896597\n",
      "Epoch 4000, Loss: 0.007367235687730666\n",
      "Epoch 5000, Loss: 0.0049827898198305885\n",
      "Epoch 5000, Loss: 0.0049827898198305885\n",
      "Epoch 6000, Loss: 0.0037203350126468002\n",
      "Epoch 7000, Loss: 0.002949367487657463\n",
      "Epoch 8000, Loss: 0.002433581867495\n",
      "Epoch 9000, Loss: 0.00206602256876797\n",
      "Epoch 6000, Loss: 0.0037203350126468002\n",
      "Epoch 7000, Loss: 0.002949367487657463\n",
      "Epoch 8000, Loss: 0.002433581867495\n",
      "Epoch 9000, Loss: 0.00206602256876797\n",
      "\n",
      "Prediksi (3 neuron, Sigmoid):\n",
      "[[0.04281649]\n",
      " [0.95472355]\n",
      " [0.95778877]\n",
      " [0.03876532]]\n",
      "\n",
      "Prediksi (3 neuron, Sigmoid):\n",
      "[[0.04281649]\n",
      " [0.95472355]\n",
      " [0.95778877]\n",
      " [0.03876532]]\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi dengan 3 neuron hidden layer\n",
    "hidden_size_exp1 = 3\n",
    "W1_exp1 = np.random.randn(input_size, hidden_size_exp1)\n",
    "b1_exp1 = np.zeros((1, hidden_size_exp1))\n",
    "W2_exp1 = np.random.randn(hidden_size_exp1, output_size)\n",
    "b2_exp1 = np.zeros((1, output_size))\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1_exp1) + b1_exp1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2_exp1) + b2_exp1\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2_exp1.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1_exp1 += lr * d_W1\n",
    "    b1_exp1 += lr * d_b1\n",
    "    W2_exp1 += lr * d_W2\n",
    "    b2_exp1 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"\\nPrediksi (3 neuron, Sigmoid):\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Inisialisasi bobot untuk 3 neuron hidden layer (`hidden_size_exp1 = 3`)\n",
    "- Proses training sama dengan Praktikum 1: forward pass → hitung error → backpropagation → update bobot\n",
    "- Menggunakan fungsi `sigmoid()` dan `sigmoid_derivative()` yang sudah didefinisikan sebelumnya\n",
    "- Loss dicetak setiap 1000 epoch untuk monitoring\n",
    "\n",
    "**Analisis Output:**\n",
    "Loss akan berkurang secara bertahap dari nilai tinggi di epoch awal menuju mendekati 0. Dengan 3 neuron, jaringan memiliki kapasitas lebih besar untuk mempelajari pola XOR, berpotensi menghasilkan loss akhir yang lebih rendah dibanding konfigurasi 2 neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksperimen 2: Hidden Layer 3 Neuron (ReLU)\n",
    "\n",
    "Eksperimen kedua menguji fungsi aktivasi ReLU (Rectified Linear Unit) pada hidden layer sebagai alternatif dari Sigmoid. ReLU sering memberikan konvergensi lebih cepat karena tidak mengalami vanishing gradient problem seperti Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.36044020089888434\n",
      "Epoch 1000, Loss: 0.16693945243915095\n",
      "Epoch 2000, Loss: 0.16677230790397726\n",
      "Epoch 3000, Loss: 0.16672926486370704\n",
      "Epoch 4000, Loss: 0.16671053091833424\n",
      "Epoch 1000, Loss: 0.16693945243915095\n",
      "Epoch 2000, Loss: 0.16677230790397726\n",
      "Epoch 3000, Loss: 0.16672926486370704\n",
      "Epoch 4000, Loss: 0.16671053091833424\n",
      "Epoch 5000, Loss: 0.16670370481875338\n",
      "Epoch 5000, Loss: 0.16670370481875338\n",
      "Epoch 6000, Loss: 0.16669549208263237\n",
      "Epoch 7000, Loss: 0.166690910763336\n",
      "Epoch 8000, Loss: 0.16668830070798663\n",
      "Epoch 9000, Loss: 0.16668701425420684\n",
      "Epoch 6000, Loss: 0.16669549208263237\n",
      "Epoch 7000, Loss: 0.166690910763336\n",
      "Epoch 8000, Loss: 0.16668830070798663\n",
      "Epoch 9000, Loss: 0.16668701425420684\n",
      "\n",
      "Prediksi (3 neuron, ReLU):\n",
      "[[0.33342759]\n",
      " [0.33342759]\n",
      " [0.99210544]\n",
      " [0.33342759]]\n",
      "\n",
      "Prediksi (3 neuron, ReLU):\n",
      "[[0.33342759]\n",
      " [0.33342759]\n",
      " [0.99210544]\n",
      " [0.33342759]]\n"
     ]
    }
   ],
   "source": [
    "# Fungsi aktivasi ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Inisialisasi dengan 3 neuron hidden layer\n",
    "W1_exp2 = np.random.randn(input_size, hidden_size_exp1)\n",
    "b1_exp2 = np.zeros((1, hidden_size_exp1))\n",
    "W2_exp2 = np.random.randn(hidden_size_exp1, output_size)\n",
    "b2_exp2 = np.zeros((1, output_size))\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass (ReLU di hidden layer)\n",
    "    z1 = np.dot(X, W1_exp2) + b1_exp2\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2_exp2) + b2_exp2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2_exp2.T) * relu_derivative(z1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1_exp2 += lr * d_W1\n",
    "    b1_exp2 += lr * d_b1\n",
    "    W2_exp2 += lr * d_W2\n",
    "    b2_exp2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"\\nPrediksi (3 neuron, ReLU):\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Mendefinisikan fungsi `relu()` yang mengembalikan max(0, x) dan `relu_derivative()` untuk backpropagation\n",
    "- Hidden layer menggunakan aktivasi ReLU, sedangkan output layer tetap menggunakan Sigmoid\n",
    "- Pada backpropagation, turunan ReLU diterapkan pada hidden layer: `relu_derivative(z1)`\n",
    "- Struktur training tetap sama: forward pass → error → backpropagation → update\n",
    "\n",
    "**Analisis Output:**\n",
    "ReLU cenderung memberikan gradien yang lebih stabil (tidak mendekati 0 seperti Sigmoid), sehingga training bisa lebih cepat. Loss akan menurun dengan pola yang mungkin berbeda dari Sigmoid—bisa lebih cepat konvergen atau bahkan lebih stabil tergantung inisialisasi bobot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perbandingan Hasil\n",
    "\n",
    "Bagian ini menghitung dan membandingkan loss akhir dari ketiga konfigurasi untuk melihat pengaruh jumlah neuron dan fungsi aktivasi terhadap performa jaringan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Konfigurasi  Hidden Neurons Aktivasi Hidden  Loss Akhir\n",
      "2 Neuron + Sigmoid               2         Sigmoid    0.002279\n",
      "3 Neuron + Sigmoid               3         Sigmoid    0.001792\n",
      "   3 Neuron + ReLU               3            ReLU    0.166682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Hitung loss akhir untuk setiap konfigurasi\n",
    "# Konfigurasi awal (2 neuron, Sigmoid) - dari Praktikum 1\n",
    "z1_orig = np.dot(X, W1) + b1\n",
    "a1_orig = sigmoid(z1_orig)\n",
    "z2_orig = np.dot(a1_orig, W2) + b2\n",
    "a2_orig = sigmoid(z2_orig)\n",
    "loss_orig = np.mean(np.square(y - a2_orig))\n",
    "\n",
    "# Eksperimen 1 (3 neuron, Sigmoid)\n",
    "z1_exp1 = np.dot(X, W1_exp1) + b1_exp1\n",
    "a1_exp1 = sigmoid(z1_exp1)\n",
    "z2_exp1 = np.dot(a1_exp1, W2_exp1) + b2_exp1\n",
    "a2_exp1 = sigmoid(z2_exp1)\n",
    "loss_exp1 = np.mean(np.square(y - a2_exp1))\n",
    "\n",
    "# Eksperimen 2 (3 neuron, ReLU)\n",
    "z1_exp2 = np.dot(X, W1_exp2) + b1_exp2\n",
    "a1_exp2 = relu(z1_exp2)\n",
    "z2_exp2 = np.dot(a1_exp2, W2_exp2) + b2_exp2\n",
    "a2_exp2 = sigmoid(z2_exp2)\n",
    "loss_exp2 = np.mean(np.square(y - a2_exp2))\n",
    "\n",
    "# Buat tabel perbandingan\n",
    "comparison = pd.DataFrame({\n",
    "    'Konfigurasi': ['2 Neuron + Sigmoid', '3 Neuron + Sigmoid', '3 Neuron + ReLU'],\n",
    "    'Hidden Neurons': [2, 3, 3],\n",
    "    'Aktivasi Hidden': ['Sigmoid', 'Sigmoid', 'ReLU'],\n",
    "    'Loss Akhir': [loss_orig, loss_exp1, loss_exp2]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Melakukan forward pass ulang untuk setiap konfigurasi menggunakan bobot yang sudah dilatih\n",
    "- Menghitung Mean Squared Error (MSE) sebagai loss akhir untuk masing-masing model\n",
    "- Membuat tabel perbandingan menggunakan pandas DataFrame\n",
    "- Menampilkan hasil dalam format tabel yang mudah dibaca\n",
    "\n",
    "**Analisis Output:**\n",
    "Tabel akan menampilkan 3 konfigurasi dengan nilai loss masing-masing:\n",
    "- **2 Neuron + Sigmoid**: Baseline dari Praktikum 1\n",
    "- **3 Neuron + Sigmoid**: Diharapkan loss lebih rendah karena kapasitas lebih besar\n",
    "- **3 Neuron + ReLU**: Performa tergantung karakteristik ReLU—bisa lebih baik atau setara dengan Sigmoid\n",
    "\n",
    "Konfigurasi dengan loss terendah menunjukkan arsitektur paling optimal untuk problem XOR ini."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOZchEZ71eQIQoc06jHHqwP",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
