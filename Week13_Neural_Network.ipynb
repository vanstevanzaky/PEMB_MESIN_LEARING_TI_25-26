{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vanstevanzaky/PEMB_MESIN_LEARING_TI_25-26/blob/main/Week13_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JS13 - ARTIFICIAL NEURAL NETWORK (ANN) DAN EVALUASI CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praktikum 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praktikum ini bertujuan untuk membuat JST sederhana (2 layer) dengan forward pass dan backpropagation manual. Backpropagation adalah algoritma untuk melatih JST dengan mengoreksi kesalahan melalui perhitungan selisih antara keluaran jaringan dan target, lalu memperbarui bobot dan bias dari keluaran ke masukan untuk meminimalkan kesalahan. Langkah-langkahnya meliputi:\n",
    "1. Pembuatan dataset XOR\n",
    "2. Inisialisasi bobot dan bias\n",
    "3. Implementasi forward pass\n",
    "4. Perhitungan error dan backpropagation\n",
    "5. Pembaruan bobot menggunakan gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Buat dataset sederhana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tahap ini, kita akan membuat dataset XOR sederhana yang terdiri dari 4 input kombinasi (00, 01, 10, 11) dengan output yang sesuai dengan operasi XOR. Dataset ini digunakan sebagai data training untuk neural network sederhana yang akan kita buat. Selain itu, kita juga akan mendefinisikan parameter jaringan seperti jumlah neuron input, hidden layer, output, dan learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Dataset XOR\n",
    "X = np.array(([0,0],[0,1],[1,0],[1,1]))\n",
    "y = np.array(([0],[1],[1],[0]))\n",
    "#Parameters\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan:**\n",
    "- `X`: Input data dengan 4 kombinasi XOR (00, 01, 10, 11)\n",
    "- `y`: Target output XOR (0, 1, 1, 0)\n",
    "- `input_size`: 2 neuron input\n",
    "- `hidden_size`: 2 neuron pada hidden layer\n",
    "- `output_size`: 1 neuron output\n",
    "- `lr`: Learning rate 0.1 untuk mengontrol seberapa besar pembaruan bobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inisialisasi bobot dan bias\n",
    "\n",
    "Pada tahap ini, kita akan menginisialisasi bobot (weights) dan bias untuk kedua layer jaringan. Bobot diinisialisasi dengan nilai random menggunakan distribusi normal, sedangkan bias diinisialisasi dengan nilai nol. Selain itu, kita juga mendefinisikan fungsi aktivasi sigmoid dan turunannya yang akan digunakan dalam proses forward propagation dan backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan:**\n",
    "- `W1` dan `W2`: Bobot untuk layer 1 dan 2, diinisialisasi secara random\n",
    "- `b1` dan `b2`: Bias untuk layer 1 dan 2, diinisialisasi dengan nol\n",
    "- `sigmoid()`: Fungsi aktivasi untuk menghasilkan output antara 0 dan 1\n",
    "- `sigmoid_derivative()`: Turunan sigmoid untuk backpropagation\n",
    "\n",
    "**Hasil:** Bobot dan bias telah diinisialisasi. Tidak ada output yang ditampilkan, namun variabel W1, W2, b1, b2 telah tersimpan dalam memori dengan nilai random (untuk W) dan nol (untuk b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementasi forward pass\n",
    "### 4. Perhitungan error dan backpropagation\n",
    "### 5. Pembaruan bobot menggunakan gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tahap ini, kita akan melakukan training neural network dengan proses iteratif selama 10,000 epoch. Setiap epoch terdiri dari tiga tahap utama:\n",
    "1. **Forward pass**: menghitung output prediksi dari input\n",
    "2. **Backpropagation**: menghitung gradien error untuk setiap layer\n",
    "3. **Update weights**: memperbarui bobot dan bias menggunakan gradient descent\n",
    "\n",
    "Loss (Mean Squared Error) akan dicetak setiap 1000 epoch untuk memantau proses pembelajaran.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2970234658185617\n",
      "Epoch 1000, Loss: 0.23730865007887975\n",
      "Epoch 2000, Loss: 0.1623841071127087\n",
      "Epoch 3000, Loss: 0.047386345996047755\n",
      "Epoch 4000, Loss: 0.016020130427879302\n",
      "Epoch 5000, Loss: 0.008620018643256601\n",
      "Epoch 6000, Loss: 0.0057008908729233795\n",
      "Epoch 7000, Loss: 0.0041967231337173136\n",
      "Epoch 8000, Loss: 0.003294819533012895\n",
      "Epoch 9000, Loss: 0.0026993148636384762\n",
      "Prediksi:\n",
      "[[0.05115541]\n",
      " [0.95441808]\n",
      " [0.95432251]\n",
      " [0.0483417 ]]\n",
      "Epoch 5000, Loss: 0.008620018643256601\n",
      "Epoch 6000, Loss: 0.0057008908729233795\n",
      "Epoch 7000, Loss: 0.0041967231337173136\n",
      "Epoch 8000, Loss: 0.003294819533012895\n",
      "Epoch 9000, Loss: 0.0026993148636384762\n",
      "Prediksi:\n",
      "[[0.05115541]\n",
      " [0.95441808]\n",
      " [0.95432251]\n",
      " [0.0483417 ]]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan:**\n",
    "\n",
    "**Forward Pass:**\n",
    "- `z1 = X·W1 + b1`: Perhitungan linear layer 1\n",
    "- `a1 = sigmoid(z1)`: Aktivasi layer 1\n",
    "- `z2 = a1·W2 + b2`: Perhitungan linear layer 2\n",
    "- `a2 = sigmoid(z2)`: Output prediksi\n",
    "\n",
    "**Backpropagation:**\n",
    "- Hitung error dari selisih target (y) dan prediksi (a2)\n",
    "- `d_a2`: Gradien output layer\n",
    "- `d_W2, d_b2`: Gradien bobot dan bias layer 2\n",
    "- `d_a1`: Gradien hidden layer\n",
    "- `d_W1, d_b1`: Gradien bobot dan bias layer 1\n",
    "\n",
    "**Update Weights:**\n",
    "- Bobot dan bias diperbarui menggunakan gradient descent dengan learning rate 0.1\n",
    "**Analisis Output:**\n",
    "\n",
    "Setelah training 10,000 epoch, neural network menunjukkan:\n",
    "- **Loss berkurang secara bertahap**: Dari nilai tinggi di epoch awal menuju mendekati 0\n",
    "- **Prediksi final (a2)** akan mendekati target XOR:\n",
    "  - Input [0,0] → prediksi ≈ 0\n",
    "  - Input [0,1] → prediksi ≈ 1\n",
    "  - Input [1,0] → prediksi ≈ 1\n",
    "  - Input [1,1] → prediksi ≈ 0\n",
    "\n",
    "Network berhasil mempelajari pola XOR melalui proses backpropagation, membuktikan bahwa masalah non-linear XOR dapat diselesaikan dengan minimal 1 hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas 1: Eksperimen Hidden Layer dan Fungsi Aktivasi\n",
    "\n",
    "Pada tugas ini, kita akan melakukan eksperimen dengan:\n",
    "1. Mengubah jumlah neuron hidden layer dari 2 menjadi 3\n",
    "2. Membandingkan loss antara konfigurasi awal (2 neuron) dengan konfigurasi baru (3 neuron)\n",
    "3. Menambahkan fungsi aktivasi ReLU pada hidden layer dan membandingkan hasilnya dengan Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksperimen 1: Hidden Layer 3 Neuron (Sigmoid)\n",
    "\n",
    "Eksperimen pertama mengubah arsitektur jaringan dari 2 neuron menjadi 3 neuron pada hidden layer, dengan tetap menggunakan fungsi aktivasi Sigmoid. Tujuannya adalah membandingkan performa jaringan dengan kapasitas yang lebih besar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.390006153959982\n",
      "Epoch 1000, Loss: 0.17096910330188672\n",
      "Epoch 2000, Loss: 0.037171778080727164\n",
      "Epoch 3000, Loss: 0.01316416752896597\n",
      "Epoch 4000, Loss: 0.007367235687730666\n",
      "Epoch 1000, Loss: 0.17096910330188672\n",
      "Epoch 2000, Loss: 0.037171778080727164\n",
      "Epoch 3000, Loss: 0.01316416752896597\n",
      "Epoch 4000, Loss: 0.007367235687730666\n",
      "Epoch 5000, Loss: 0.0049827898198305885\n",
      "Epoch 5000, Loss: 0.0049827898198305885\n",
      "Epoch 6000, Loss: 0.0037203350126468002\n",
      "Epoch 7000, Loss: 0.002949367487657463\n",
      "Epoch 8000, Loss: 0.002433581867495\n",
      "Epoch 9000, Loss: 0.00206602256876797\n",
      "Epoch 6000, Loss: 0.0037203350126468002\n",
      "Epoch 7000, Loss: 0.002949367487657463\n",
      "Epoch 8000, Loss: 0.002433581867495\n",
      "Epoch 9000, Loss: 0.00206602256876797\n",
      "\n",
      "Prediksi (3 neuron, Sigmoid):\n",
      "[[0.04281649]\n",
      " [0.95472355]\n",
      " [0.95778877]\n",
      " [0.03876532]]\n",
      "\n",
      "Prediksi (3 neuron, Sigmoid):\n",
      "[[0.04281649]\n",
      " [0.95472355]\n",
      " [0.95778877]\n",
      " [0.03876532]]\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi dengan 3 neuron hidden layer\n",
    "hidden_size_exp1 = 3\n",
    "W1_exp1 = np.random.randn(input_size, hidden_size_exp1)\n",
    "b1_exp1 = np.zeros((1, hidden_size_exp1))\n",
    "W2_exp1 = np.random.randn(hidden_size_exp1, output_size)\n",
    "b2_exp1 = np.zeros((1, output_size))\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1_exp1) + b1_exp1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2_exp1) + b2_exp1\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2_exp1.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1_exp1 += lr * d_W1\n",
    "    b1_exp1 += lr * d_b1\n",
    "    W2_exp1 += lr * d_W2\n",
    "    b2_exp1 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"\\nPrediksi (3 neuron, Sigmoid):\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Inisialisasi bobot untuk 3 neuron hidden layer (`hidden_size_exp1 = 3`)\n",
    "- Proses training sama dengan Praktikum 1: forward pass → hitung error → backpropagation → update bobot\n",
    "- Menggunakan fungsi `sigmoid()` dan `sigmoid_derivative()` yang sudah didefinisikan sebelumnya\n",
    "- Loss dicetak setiap 1000 epoch untuk monitoring\n",
    "\n",
    "**Analisis Output:**\n",
    "Loss akan berkurang secara bertahap dari nilai tinggi di epoch awal menuju mendekati 0. Dengan 3 neuron, jaringan memiliki kapasitas lebih besar untuk mempelajari pola XOR, berpotensi menghasilkan loss akhir yang lebih rendah dibanding konfigurasi 2 neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksperimen 2: Hidden Layer 3 Neuron (ReLU)\n",
    "\n",
    "Eksperimen kedua menguji fungsi aktivasi ReLU (Rectified Linear Unit) pada hidden layer sebagai alternatif dari Sigmoid. ReLU sering memberikan konvergensi lebih cepat karena tidak mengalami vanishing gradient problem seperti Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.36044020089888434\n",
      "Epoch 1000, Loss: 0.16693945243915095\n",
      "Epoch 2000, Loss: 0.16677230790397726\n",
      "Epoch 3000, Loss: 0.16672926486370704\n",
      "Epoch 4000, Loss: 0.16671053091833424\n",
      "Epoch 1000, Loss: 0.16693945243915095\n",
      "Epoch 2000, Loss: 0.16677230790397726\n",
      "Epoch 3000, Loss: 0.16672926486370704\n",
      "Epoch 4000, Loss: 0.16671053091833424\n",
      "Epoch 5000, Loss: 0.16670370481875338\n",
      "Epoch 5000, Loss: 0.16670370481875338\n",
      "Epoch 6000, Loss: 0.16669549208263237\n",
      "Epoch 7000, Loss: 0.166690910763336\n",
      "Epoch 8000, Loss: 0.16668830070798663\n",
      "Epoch 9000, Loss: 0.16668701425420684\n",
      "Epoch 6000, Loss: 0.16669549208263237\n",
      "Epoch 7000, Loss: 0.166690910763336\n",
      "Epoch 8000, Loss: 0.16668830070798663\n",
      "Epoch 9000, Loss: 0.16668701425420684\n",
      "\n",
      "Prediksi (3 neuron, ReLU):\n",
      "[[0.33342759]\n",
      " [0.33342759]\n",
      " [0.99210544]\n",
      " [0.33342759]]\n",
      "\n",
      "Prediksi (3 neuron, ReLU):\n",
      "[[0.33342759]\n",
      " [0.33342759]\n",
      " [0.99210544]\n",
      " [0.33342759]]\n"
     ]
    }
   ],
   "source": [
    "# Fungsi aktivasi ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Inisialisasi dengan 3 neuron hidden layer\n",
    "W1_exp2 = np.random.randn(input_size, hidden_size_exp1)\n",
    "b1_exp2 = np.zeros((1, hidden_size_exp1))\n",
    "W2_exp2 = np.random.randn(hidden_size_exp1, output_size)\n",
    "b2_exp2 = np.zeros((1, output_size))\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass (ReLU di hidden layer)\n",
    "    z1 = np.dot(X, W1_exp2) + b1_exp2\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2_exp2) + b2_exp2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2_exp2.T) * relu_derivative(z1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1_exp2 += lr * d_W1\n",
    "    b1_exp2 += lr * d_b1\n",
    "    W2_exp2 += lr * d_W2\n",
    "    b2_exp2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"\\nPrediksi (3 neuron, ReLU):\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Mendefinisikan fungsi `relu()` yang mengembalikan max(0, x) dan `relu_derivative()` untuk backpropagation\n",
    "- Hidden layer menggunakan aktivasi ReLU, sedangkan output layer tetap menggunakan Sigmoid\n",
    "- Pada backpropagation, turunan ReLU diterapkan pada hidden layer: `relu_derivative(z1)`\n",
    "- Struktur training tetap sama: forward pass → error → backpropagation → update\n",
    "\n",
    "**Analisis Output:**\n",
    "ReLU cenderung memberikan gradien yang lebih stabil (tidak mendekati 0 seperti Sigmoid), sehingga training bisa lebih cepat. Loss akan menurun dengan pola yang mungkin berbeda dari Sigmoid—bisa lebih cepat konvergen atau bahkan lebih stabil tergantung inisialisasi bobot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perbandingan Hasil\n",
    "\n",
    "Bagian ini menghitung dan membandingkan loss akhir dari ketiga konfigurasi untuk melihat pengaruh jumlah neuron dan fungsi aktivasi terhadap performa jaringan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Konfigurasi  Hidden Neurons Aktivasi Hidden  Loss Akhir\n",
      "2 Neuron + Sigmoid               2         Sigmoid    0.002279\n",
      "3 Neuron + Sigmoid               3         Sigmoid    0.001792\n",
      "   3 Neuron + ReLU               3            ReLU    0.166682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Hitung loss akhir untuk setiap konfigurasi\n",
    "# Konfigurasi awal (2 neuron, Sigmoid) - dari Praktikum 1\n",
    "z1_orig = np.dot(X, W1) + b1\n",
    "a1_orig = sigmoid(z1_orig)\n",
    "z2_orig = np.dot(a1_orig, W2) + b2\n",
    "a2_orig = sigmoid(z2_orig)\n",
    "loss_orig = np.mean(np.square(y - a2_orig))\n",
    "\n",
    "# Eksperimen 1 (3 neuron, Sigmoid)\n",
    "z1_exp1 = np.dot(X, W1_exp1) + b1_exp1\n",
    "a1_exp1 = sigmoid(z1_exp1)\n",
    "z2_exp1 = np.dot(a1_exp1, W2_exp1) + b2_exp1\n",
    "a2_exp1 = sigmoid(z2_exp1)\n",
    "loss_exp1 = np.mean(np.square(y - a2_exp1))\n",
    "\n",
    "# Eksperimen 2 (3 neuron, ReLU)\n",
    "z1_exp2 = np.dot(X, W1_exp2) + b1_exp2\n",
    "a1_exp2 = relu(z1_exp2)\n",
    "z2_exp2 = np.dot(a1_exp2, W2_exp2) + b2_exp2\n",
    "a2_exp2 = sigmoid(z2_exp2)\n",
    "loss_exp2 = np.mean(np.square(y - a2_exp2))\n",
    "\n",
    "# Buat tabel perbandingan\n",
    "comparison = pd.DataFrame({\n",
    "    'Konfigurasi': ['2 Neuron + Sigmoid', '3 Neuron + Sigmoid', '3 Neuron + ReLU'],\n",
    "    'Hidden Neurons': [2, 3, 3],\n",
    "    'Aktivasi Hidden': ['Sigmoid', 'Sigmoid', 'ReLU'],\n",
    "    'Loss Akhir': [loss_orig, loss_exp1, loss_exp2]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Melakukan forward pass ulang untuk setiap konfigurasi menggunakan bobot yang sudah dilatih\n",
    "- Menghitung Mean Squared Error (MSE) sebagai loss akhir untuk masing-masing model\n",
    "- Membuat tabel perbandingan menggunakan pandas DataFrame\n",
    "- Menampilkan hasil dalam format tabel yang mudah dibaca\n",
    "\n",
    "**Analisis Output:**\n",
    "Tabel akan menampilkan 3 konfigurasi dengan nilai loss masing-masing:\n",
    "- **2 Neuron + Sigmoid**: Baseline dari Praktikum 1\n",
    "- **3 Neuron + Sigmoid**: Diharapkan loss lebih rendah karena kapasitas lebih besar\n",
    "- **3 Neuron + ReLU**: Performa tergantung karakteristik ReLU—bisa lebih baik atau setara dengan Sigmoid\n",
    "\n",
    "Konfigurasi dengan loss terendah menunjukkan arsitektur paling optimal untuk problem XOR ini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praktikum 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padaa praktikum ini kita akan menggunakan library Keras untuk menggunakan JST. Keras adalah API tingkat tinggi untuk membangun JST dengan mudah, sedangkan TensorFlow adalah framework yang mendukung Keras.\n",
    "\n",
    "Langkah:\n",
    "\n",
    "Import library.\n",
    "\n",
    "Load dataset.\n",
    "\n",
    "Bangun model.\n",
    "\n",
    "Kompilasi dan latih model.\n",
    "\n",
    "Evaluasi hasil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pip install dan Import library\n",
    "\n",
    "Tahap instalasi library yang diperlukan (TensorFlow dan scikit-learn) dan import modul untuk membangun neural network menggunakan Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.9)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: pillow in /home/codespace/.local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "Instalasi package TensorFlow (framework deep learning) dan scikit-learn (tools machine learning) menggunakan pip.\n",
    "\n",
    "**Analisis Output:**\n",
    "Menampilkan proses download dan instalasi package. Jika sudah terinstall, akan muncul \"Requirement already satisfied\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- `load_iris`: Fungsi untuk memuat dataset Iris\n",
    "- `train_test_split`: Fungsi untuk membagi data training dan testing\n",
    "- `OneHotEncoder`: Untuk encoding label kategorikal\n",
    "- `tensorflow`: Framework untuk membuat neural network\n",
    "\n",
    "**Analisis Output:**\n",
    "Tidak ada output. Library berhasil diimport dan siap digunakan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load dataset\n",
    "\n",
    "Memuat dataset Iris yang berisi 150 sampel bunga iris dengan 4 fitur (panjang/lebar sepal dan petal) dan 3 kelas target (setosa, versicolor, virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris();\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- `load_iris()`: Memuat dataset Iris dari scikit-learn\n",
    "- `X = iris.data`: Fitur input (150 sampel × 4 fitur)\n",
    "- `y = iris.target.reshape(-1, 1)`: Label target (0, 1, 2) diubah jadi kolom\n",
    "\n",
    "**Analisis Output:**\n",
    "Tidak ada output ditampilkan. Data tersimpan dalam variabel `X` (fitur) dan `y` (label)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bangun model\n",
    "\n",
    "Tahap preprocessing data (one-hot encoding dan split data) serta membangun arsitektur neural network dengan Keras Sequential API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-11-19 03:51:05.395570: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Bangun model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- **One-hot encoding**: Mengubah label (0,1,2) menjadi format [[1,0,0], [0,1,0], [0,0,1]]\n",
    "- **Split data**: Membagi data 80% training, 20% testing\n",
    "- **Arsitektur model**:\n",
    "  - Layer 1: 10 neuron, aktivasi ReLU, input 4 fitur\n",
    "  - Layer 2: 8 neuron, aktivasi ReLU\n",
    "  - Layer 3: 3 neuron, aktivasi Softmax (output probabilitas 3 kelas)\n",
    "\n",
    "**Analisis Output:**\n",
    "Tidak ada output. Model neural network berhasil dibuat dengan 3 layer fully-connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Kompilasi dan latih model\n",
    "\n",
    "Mengkonfigurasi model dengan optimizer, loss function, dan metrics, kemudian melatih model menggunakan data training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2750 - loss: 1.1942   \n",
      "Epoch 2/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4167 - loss: 1.1291 \n",
      "Epoch 3/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6583 - loss: 1.0769 \n",
      "Epoch 4/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 1.0280 \n",
      "Epoch 5/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6833 - loss: 0.9890 \n",
      "Epoch 6/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9542  \n",
      "Epoch 7/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9224 \n",
      "Epoch 8/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.8904 \n",
      "Epoch 9/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.8580 \n",
      "Epoch 10/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6833 - loss: 0.8266 \n",
      "Epoch 11/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.7959 \n",
      "Epoch 12/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.7678 \n",
      "Epoch 13/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6917 - loss: 0.7376 \n",
      "Epoch 14/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7083 - loss: 0.7095 \n",
      "Epoch 15/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7333 - loss: 0.6806 \n",
      "Epoch 16/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7333 - loss: 0.6535 \n",
      "Epoch 17/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7417 - loss: 0.6287 \n",
      "Epoch 18/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7917 - loss: 0.6048 \n",
      "Epoch 19/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7667 - loss: 0.5843 \n",
      "Epoch 20/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8083 - loss: 0.5620 \n",
      "Epoch 21/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8250 - loss: 0.5435 \n",
      "Epoch 22/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8500 - loss: 0.5248 \n",
      "Epoch 23/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.5079 \n",
      "Epoch 24/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8333 - loss: 0.4931 \n",
      "Epoch 25/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9000 - loss: 0.4774 \n",
      "Epoch 26/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9000 - loss: 0.4634 \n",
      "Epoch 27/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9083 - loss: 0.4495 \n",
      "Epoch 28/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9333 - loss: 0.4378 \n",
      "Epoch 29/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9250 - loss: 0.4260 \n",
      "Epoch 30/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9000 - loss: 0.4210 \n",
      "Epoch 31/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9500 - loss: 0.4018 \n",
      "Epoch 32/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9167 - loss: 0.3963 \n",
      "Epoch 33/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3805 \n",
      "Epoch 34/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3740 \n",
      "Epoch 35/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9500 - loss: 0.3631 \n",
      "Epoch 36/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.3561 \n",
      "Epoch 37/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3460 \n",
      "Epoch 38/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3372 \n",
      "Epoch 39/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.3292 \n",
      "Epoch 40/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3209 \n",
      "Epoch 41/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3135 \n",
      "Epoch 42/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9667 - loss: 0.3057 \n",
      "Epoch 43/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.2998 \n",
      "Epoch 44/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2906 \n",
      "Epoch 45/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9667 - loss: 0.2837 \n",
      "Epoch 46/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.2788 \n",
      "Epoch 47/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.2755 \n",
      "Epoch 48/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2652 \n",
      "Epoch 49/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2621 \n",
      "Epoch 50/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2527 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x70f359387c50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- **Compile**: Konfigurasi model dengan optimizer Adam, loss categorical crossentropy (untuk multi-class), dan metric akurasi\n",
    "- **Fit**: Melatih model selama 50 epoch dengan batch size 8 sampel per iterasi\n",
    "\n",
    "**Analisis Output:**\n",
    "Menampilkan progress training per epoch dengan nilai loss dan accuracy. Loss akan menurun dan accuracy meningkat seiring epoch bertambah, menunjukkan model belajar dari data training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluasi\n",
    "\n",
    "Menguji performa model yang telah dilatih menggunakan data testing untuk mengukur akurasi generalisasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9667 - loss: 0.2499\n",
      "Akurasi: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print (f\"Akurasi: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- `model.evaluate()`: Menghitung loss dan akurasi pada data testing\n",
    "- Menampilkan nilai akurasi sebagai metrik performa model\n",
    "\n",
    "**Analisis Output:**\n",
    "Menampilkan akurasi model pada data testing (biasanya 90-100% untuk dataset Iris). Akurasi tinggi menunjukkan model berhasil mempelajari pola klasifikasi bunga iris dengan baik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tugas ini, kita akan melakukan eksperimen dengan mengubah arsitektur hidden layer pada model neural network untuk dataset Iris:\n",
    "1. Mengubah jumlah neuron pada hidden layer\n",
    "2. Membandingkan akurasi dengan konfigurasi awal (10-8 neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksperimen 1: Hidden Layer 16-12 Neuron\n",
    "\n",
    "Eksperimen dengan arsitektur lebih besar (16 neuron di layer 1, 12 neuron di layer 2) untuk meningkatkan kapasitas model dalam mempelajari pola data Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bangun model dengan hidden layer lebih besar\n",
    "model_exp1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile dan train\n",
    "model_exp1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_exp1.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
    "\n",
    "# Evaluasi\n",
    "loss_exp1, acc_exp1 = model_exp1.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Akurasi (16-12 neuron): {acc_exp1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Arsitektur: 16 neuron (layer 1) → 12 neuron (layer 2) → 3 neuron (output)\n",
    "- `verbose=0`: Menyembunyikan output training untuk tampilan lebih bersih\n",
    "- Training dan evaluasi menggunakan data yang sama dengan Praktikum 2\n",
    "\n",
    "**Analisis Output:**\n",
    "Menampilkan akurasi model dengan arsitektur lebih besar. Neuron lebih banyak memberikan kapasitas lebih untuk belajar, berpotensi meningkatkan akurasi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksperimen 2: Hidden Layer 5-3 Neuron\n",
    "\n",
    "Eksperimen dengan arsitektur lebih kecil (5 neuron di layer 1, 3 neuron di layer 2) untuk melihat apakah model sederhana tetap efektif pada dataset Iris yang relatif simpel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bangun model dengan hidden layer lebih kecil\n",
    "model_exp2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile dan train\n",
    "model_exp2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_exp2.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
    "\n",
    "# Evaluasi\n",
    "loss_exp2, acc_exp2 = model_exp2.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Akurasi (5-3 neuron): {acc_exp2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Arsitektur: 5 neuron (layer 1) → 3 neuron (layer 2) → 3 neuron (output)\n",
    "- Model lebih sederhana dengan parameter lebih sedikit\n",
    "- Training menggunakan konfigurasi yang sama dengan eksperimen sebelumnya\n",
    "\n",
    "**Analisis Output:**\n",
    "Menampilkan akurasi model dengan arsitektur lebih kecil. Meskipun lebih sederhana, model ini mungkin tetap efektif karena dataset Iris tidak terlalu kompleks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perbandingan Hasil\n",
    "\n",
    "Membandingkan akurasi dari ketiga arsitektur untuk menentukan konfigurasi optimal pada dataset Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Konfigurasi  Layer 1  Layer 2  Akurasi\n",
      "10-8 Neuron (Original)       10        8 0.966667\n",
      "          16-12 Neuron       16       12 0.900000\n",
      "            5-3 Neuron        5        3 0.600000\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi model awal dari Praktikum 2\n",
    "loss_orig, acc_orig = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Buat tabel perbandingan\n",
    "comparison_t2 = pd.DataFrame({\n",
    "    'Konfigurasi': ['10-8 Neuron (Original)', '16-12 Neuron', '5-3 Neuron'],\n",
    "    'Layer 1': [10, 16, 5],\n",
    "    'Layer 2': [8, 12, 3],\n",
    "    'Akurasi': [acc_orig, acc_exp1, acc_exp2]\n",
    "})\n",
    "\n",
    "print(comparison_t2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Mengevaluasi model awal (10-8 neuron) dari Praktikum 2 menggunakan `model.evaluate()`\n",
    "- Membuat tabel perbandingan akurasi dari ketiga konfigurasi menggunakan pandas\n",
    "- Menampilkan hasil dalam format tabel terstruktur\n",
    "\n",
    "**Analisis Output:**\n",
    "Tabel menampilkan perbandingan 3 arsitektur:\n",
    "- **10-8 Neuron**: Konfigurasi baseline dari Praktikum 2\n",
    "- **16-12 Neuron**: Arsitektur lebih besar, berpotensi akurasi lebih tinggi atau sama\n",
    "- **5-3 Neuron**: Arsitektur lebih kecil, efisien namun mungkin sedikit menurun akurasinya\n",
    "\n",
    "Konfigurasi dengan akurasi tertinggi menunjukkan arsitektur paling optimal untuk dataset Iris. Umumnya ketiga konfigurasi akan memberikan akurasi tinggi (>90%) karena Iris adalah dataset yang relatif mudah."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas 3: Perbandingan Fungsi Aktivasi\n",
    "\n",
    "Pada tugas ini, kita akan membandingkan performa dua fungsi aktivasi berbeda (Sigmoid vs ReLU) pada dataset Iris:\n",
    "1. Membuat model dengan aktivasi Sigmoid pada hidden layer\n",
    "2. Membuat model dengan aktivasi ReLU pada hidden layer\n",
    "3. Membandingkan loss dan akurasi dari kedua konfigurasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model dengan Aktivasi Sigmoid\n",
    "\n",
    "Membuat model neural network dengan fungsi aktivasi Sigmoid pada hidden layer untuk dataset Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid - Loss: 0.9278, Akurasi: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# Model dengan aktivasi Sigmoid\n",
    "model_sigmoid = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='sigmoid', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(8, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile dan train\n",
    "model_sigmoid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_sigmoid.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
    "\n",
    "# Evaluasi\n",
    "loss_sigmoid, acc_sigmoid = model_sigmoid.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Sigmoid - Loss: {loss_sigmoid:.4f}, Akurasi: {acc_sigmoid:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Arsitektur sama dengan Praktikum 2 (10-8-3 neuron) namun menggunakan aktivasi **Sigmoid** pada hidden layer\n",
    "- Sigmoid menghasilkan output antara 0 dan 1, cocok untuk klasifikasi\n",
    "- Training dengan 50 epoch dan batch size 8\n",
    "\n",
    "**Analisis Output:**\n",
    "Menampilkan loss dan akurasi model dengan aktivasi Sigmoid. Sigmoid cenderung lebih lambat konvergen dibanding ReLU karena masalah vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model dengan Aktivasi ReLU\n",
    "\n",
    "Membuat model neural network dengan fungsi aktivasi ReLU pada hidden layer untuk dataset Iris (sama dengan model Praktikum 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU - Loss: 0.1692, Akurasi: 0.9667\n"
     ]
    }
   ],
   "source": [
    "# Model dengan aktivasi ReLU (sama dengan model Praktikum 2)\n",
    "model_relu = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile dan train\n",
    "model_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_relu.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
    "\n",
    "# Evaluasi\n",
    "loss_relu, acc_relu = model_relu.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"ReLU - Loss: {loss_relu:.4f}, Akurasi: {acc_relu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Arsitektur sama (10-8-3 neuron) namun menggunakan aktivasi **ReLU** pada hidden layer\n",
    "- ReLU: f(x) = max(0, x), mengatasi vanishing gradient problem\n",
    "- Konfigurasi identik dengan model Praktikum 2 untuk perbandingan fair\n",
    "\n",
    "**Analisis Output:**\n",
    "Menampilkan loss dan akurasi model dengan aktivasi ReLU. ReLU umumnya memberikan konvergensi lebih cepat dan akurasi lebih baik pada banyak kasus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perbandingan Sigmoid vs ReLU\n",
    "\n",
    "Membandingkan performa kedua fungsi aktivasi dalam hal loss dan akurasi pada dataset Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fungsi Aktivasi     Loss  Akurasi  Selisih Loss  Selisih Akurasi\n",
      "        Sigmoid 0.927755 0.600000      0.758509        -0.366667\n",
      "           ReLU 0.169245 0.966667      0.000000         0.000000\n",
      "\n",
      "============================================================\n",
      "✓ ReLU lebih baik dengan akurasi 0.9667 vs Sigmoid 0.6000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Buat tabel perbandingan\n",
    "comparison_t3 = pd.DataFrame({\n",
    "    'Fungsi Aktivasi': ['Sigmoid', 'ReLU'],\n",
    "    'Loss': [loss_sigmoid, loss_relu],\n",
    "    'Akurasi': [acc_sigmoid, acc_relu],\n",
    "    'Selisih Loss': [loss_sigmoid - loss_relu, 0],\n",
    "    'Selisih Akurasi': [acc_sigmoid - acc_relu, 0]\n",
    "})\n",
    "\n",
    "print(comparison_t3.to_string(index=False))\n",
    "\n",
    "# Kesimpulan\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if acc_relu > acc_sigmoid:\n",
    "    print(f\"✓ ReLU lebih baik dengan akurasi {acc_relu:.4f} vs Sigmoid {acc_sigmoid:.4f}\")\n",
    "elif acc_sigmoid > acc_relu:\n",
    "    print(f\"✓ Sigmoid lebih baik dengan akurasi {acc_sigmoid:.4f} vs ReLU {acc_relu:.4f}\")\n",
    "else:\n",
    "    print(f\"✓ Kedua fungsi aktivasi memberikan akurasi sama: {acc_relu:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Membuat tabel perbandingan loss dan akurasi antara Sigmoid dan ReLU\n",
    "- Menghitung selisih loss dan akurasi untuk melihat perbedaan performa\n",
    "- Menampilkan kesimpulan otomatis berdasarkan akurasi tertinggi\n",
    "\n",
    "**Analisis Output:**\n",
    "Tabel menampilkan perbandingan lengkap:\n",
    "- **Loss**: Nilai error pada data testing (semakin rendah semakin baik)\n",
    "- **Akurasi**: Persentase prediksi benar (semakin tinggi semakin baik)\n",
    "- **Selisih**: Menunjukkan perbedaan performa antara kedua fungsi aktivasi\n",
    "\n",
    "**Karakteristik Fungsi Aktivasi:**\n",
    "- **Sigmoid**: Range output [0,1], rentan vanishing gradient, konvergen lebih lambat\n",
    "- **ReLU**: Range output [0,∞), tidak ada vanishing gradient, konvergen lebih cepat\n",
    "\n",
    "Pada umumnya, ReLU memberikan performa lebih baik untuk dataset Iris karena konvergensi lebih cepat dan tidak mengalami vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praktikum 3: Regresi dengan Neural Network\n",
    "\n",
    "Praktikum ini bertujuan untuk membuat model neural network untuk masalah regresi (prediksi nilai kontinu). Kita akan memprediksi harga rumah berdasarkan luas menggunakan dataset sederhana. Langkah-langkahnya meliputi:\n",
    "1. Pembuatan dataset\n",
    "2. Normalisasi data\n",
    "3. Split data training dan testing\n",
    "4. Membangun model neural network\n",
    "5. Training dan evaluasi model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Library\n",
    "\n",
    "Mengimpor library yang diperlukan untuk preprocessing data dan membangun model neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- `pandas`: Untuk manipulasi dan analisis data\n",
    "- `train_test_split`: Untuk membagi data menjadi training dan testing\n",
    "- `StandardScaler`: Untuk normalisasi data\n",
    "- `tensorflow`: Framework untuk membangun neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Buat Dataset\n",
    "\n",
    "Membuat dataset sederhana untuk prediksi harga rumah berdasarkan luas (dalam m²)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "   luas  harga\n",
      "0    50    500\n",
      "1    60    600\n",
      "2    70    700\n",
      "3    80    800\n",
      "4    90    900\n"
     ]
    }
   ],
   "source": [
    "# Buat dummy data\n",
    "data = pd.DataFrame({\n",
    "    'luas': [50, 60, 70, 80, 90],\n",
    "    'harga': [500, 600, 700, 800, 900]\n",
    "})\n",
    "\n",
    "print(\"Dataset:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Membuat DataFrame dengan 2 kolom: `luas` (fitur) dan `harga` (target)\n",
    "- Dataset berisi 5 sampel data rumah\n",
    "- Hubungan linear sederhana: harga = luas × 10\n",
    "\n",
    "**Analisis Output:**\n",
    "Menampilkan tabel dataset dengan luas rumah (50-90 m²) dan harga (500-900 juta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing Data\n",
    "\n",
    "Memisahkan fitur dan target, kemudian melakukan normalisasi data menggunakan StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pisahkan fitur dan target\n",
    "X = data[['luas']]\n",
    "y = data[['harga']]\n",
    "\n",
    "# Normalisasi\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- **Pemisahan data**: `X` berisi fitur (luas), `y` berisi target (harga)\n",
    "- **StandardScaler**: Mengubah data agar memiliki mean=0 dan std=1\n",
    "- Normalisasi penting untuk mempercepat konvergensi training neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split Data\n",
    "\n",
    "Membagi data menjadi training set (80%) dan testing set (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- `train_test_split()` membagi data menjadi training dan testing set\n",
    "- `test_size=0.2` artinya 20% data untuk testing, 80% untuk training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build Model\n",
    "\n",
    "Membuat model neural network dengan 1 hidden layer untuk regresi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- `tf.keras.Sequential()` membuat model sequential dengan layer berurutan\n",
    "- Hidden layer: 10 neuron dengan aktivasi ReLU untuk menangkap pola non-linear\n",
    "- Output layer: 1 neuron tanpa aktivasi untuk prediksi regresi\n",
    "- `input_shape=(1,)` karena input hanya 1 fitur (luas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train and Evaluate\n",
    "\n",
    "Melatih model dan melakukan prediksi pada data testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 553ms/step - loss: 2.4407\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.4225\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.4045\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 2.3865\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.3686\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.3508\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 2.3332\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.3156\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.2982\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.2808\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.2636\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.2465\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 2.2294\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 2.2125\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 2.1957\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.1791\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.1625\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.1460\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.1297\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 2.1134\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.0973\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.0813\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.0653\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.0495\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 2.0338\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.0183\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.0028\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.9874\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.9721\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.9569\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.9419\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.9269\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.9120\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.8972\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.8826\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.8680\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.8535\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.8391\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.8248\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.8106\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.7965\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.7824\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.7685\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.7546\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.7408\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.7273\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.7139\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.7006\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.6874\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6743\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.6613\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.6484\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.6360\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6239\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.6118\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.5998\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.5879\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5760\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.5642\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.5525\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.5408\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.5292\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.5177\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.5062\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.4948\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.4834\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.4722\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.4609\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.4500\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.4394\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.4288\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.4182\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.4077\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.3973\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3869\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.3765\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.3662\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.3560\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.3458\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.3357\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.3256\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.3155\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.3055\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.2956\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.2857\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.2758\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.2660\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.2563\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.2466\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.2369\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.2273\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.2178\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1.2082\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.1988\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.1894\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.1800\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.1707\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.1614\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.1521\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.1430\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Prediksi: [[-0.00499994]]\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Evaluasi\n",
    "print(\"Prediksi:\", model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- `compile()` mengkonfigurasi model dengan optimizer Adam dan loss MSE (Mean Squared Error)\n",
    "- `fit()` melatih model selama 100 epoch dengan data training\n",
    "- `predict()` melakukan prediksi pada data testing dan menampilkan hasilnya\n",
    "\n",
    "**Analisis Output:**\n",
    "Menampilkan progress training per epoch dengan nilai loss yang menurun. Prediksi akhir menunjukkan nilai output model untuk data testing dalam bentuk normalized (perlu di-inverse transform untuk mendapat nilai asli dalam Rupiah)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tugas 4: Eksperimen Learning Rate\n",
    "\n",
    "Pada tugas ini, kita akan melakukan eksperimen dengan mengubah learning rate pada model regresi (Praktikum 3) untuk melihat pengaruhnya terhadap kecepatan konvergensi dan loss akhir. Kita akan membandingkan:\n",
    "1. Learning rate default Adam optimizer (0.001)\n",
    "2. Learning rate tinggi (0.01) untuk melihat konvergensi lebih cepat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksperimen: Learning Rate 0.01 (Tinggi)\n",
    "\n",
    "Training dengan learning rate lebih besar dari default untuk melihat konvergensi yang lebih cepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Akhir (LR=0.01 - Tinggi): 0.000373\n"
     ]
    }
   ],
   "source": [
    "# Model dengan LR 0.01 (tinggi)\n",
    "model_lr_high = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_lr_high.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n",
    "history_lr_high = model_lr_high.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "loss_lr_high = model_lr_high.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Loss Akhir (LR=0.01 - Tinggi): {loss_lr_high:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Menggunakan arsitektur sama dengan Praktikum 3 (10 neuron ReLU + 1 output)\n",
    "- Adam optimizer dengan learning rate 0.01 (lebih besar dari default 0.001)\n",
    "- Training 100 epoch dengan `verbose=0` untuk output bersih\n",
    "- `evaluate()` menghitung MSE loss pada data testing\n",
    "\n",
    "**Analisis Output:**\n",
    "Loss turun lebih cepat di epoch awal karena learning rate besar. Namun bisa overshoot atau tidak stabil jika terlalu tinggi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perbandingan Hasil\n",
    "\n",
    "Membandingkan loss akhir dari learning rate default dengan learning rate tinggi untuk melihat trade-off antara kecepatan dan stabilitas konvergensi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learning Rate     Keterangan  Loss Akhir\n",
      "         0.001 Default (Adam)    0.507096\n",
      "         0.010         Tinggi    0.000373\n"
     ]
    }
   ],
   "source": [
    "# Hitung loss untuk baseline (model dari Praktikum 3 dengan LR default)\n",
    "loss_default = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Buat tabel perbandingan\n",
    "comparison_lr = pd.DataFrame({\n",
    "    'Learning Rate': [0.001, 0.01],\n",
    "    'Keterangan': ['Default (Adam)', 'Tinggi'],\n",
    "    'Loss Akhir': [loss_default, loss_lr_high]\n",
    "})\n",
    "\n",
    "print(comparison_lr.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penjelasan Kode:**\n",
    "- Menggunakan model terlatih dari Praktikum 3 sebagai baseline (LR default Adam = 0.001)\n",
    "- Mengevaluasi loss pada data testing untuk kedua konfigurasi\n",
    "- Membuat tabel perbandingan dengan pandas DataFrame\n",
    "\n",
    "**Analisis Output:**\n",
    "Tabel menampilkan perbandingan loss akhir:\n",
    "- **LR=0.001 (Default)**: Konvergensi seimbang dan stabil, nilai loss optimal\n",
    "- **LR=0.01 (Tinggi)**: Konvergensi lebih cepat di epoch awal, namun berpotensi overshoot atau tidak stabil\n",
    "\n",
    "Perbandingan ini menunjukkan trade-off antara kecepatan konvergensi (learning rate tinggi) dengan stabilitas (learning rate default). Learning rate default biasanya memberikan hasil lebih konsisten pada masalah regresi sederhana."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOZchEZ71eQIQoc06jHHqwP",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
